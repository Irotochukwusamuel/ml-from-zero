{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda4a8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.92%\n",
      "<bound method NDFrame.tail of            id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0      842302         M        17.99         10.38          122.80     1001.0   \n",
      "1      842517         M        20.57         17.77          132.90     1326.0   \n",
      "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3    84348301         M        11.42         20.38           77.58      386.1   \n",
      "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
      "..        ...       ...          ...           ...             ...        ...   \n",
      "564    926424         M        21.56         22.39          142.00     1479.0   \n",
      "565    926682         M        20.13         28.25          131.20     1261.0   \n",
      "566    926954         M        16.60         28.08          108.30      858.1   \n",
      "567    927241         M        20.60         29.33          140.10     1265.0   \n",
      "568     92751         B         7.76         24.54           47.92      181.0   \n",
      "\n",
      "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0            0.11840           0.27760         0.30010              0.14710   \n",
      "1            0.08474           0.07864         0.08690              0.07017   \n",
      "2            0.10960           0.15990         0.19740              0.12790   \n",
      "3            0.14250           0.28390         0.24140              0.10520   \n",
      "4            0.10030           0.13280         0.19800              0.10430   \n",
      "..               ...               ...             ...                  ...   \n",
      "564          0.11100           0.11590         0.24390              0.13890   \n",
      "565          0.09780           0.10340         0.14400              0.09791   \n",
      "566          0.08455           0.10230         0.09251              0.05302   \n",
      "567          0.11780           0.27700         0.35140              0.15200   \n",
      "568          0.05263           0.04362         0.00000              0.00000   \n",
      "\n",
      "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0    ...          17.33           184.60      2019.0           0.16220   \n",
      "1    ...          23.41           158.80      1956.0           0.12380   \n",
      "2    ...          25.53           152.50      1709.0           0.14440   \n",
      "3    ...          26.50            98.87       567.7           0.20980   \n",
      "4    ...          16.67           152.20      1575.0           0.13740   \n",
      "..   ...            ...              ...         ...               ...   \n",
      "564  ...          26.40           166.10      2027.0           0.14100   \n",
      "565  ...          38.25           155.00      1731.0           0.11660   \n",
      "566  ...          34.12           126.70      1124.0           0.11390   \n",
      "567  ...          39.42           184.60      1821.0           0.16500   \n",
      "568  ...          30.37            59.16       268.6           0.08996   \n",
      "\n",
      "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0              0.66560           0.7119                0.2654          0.4601   \n",
      "1              0.18660           0.2416                0.1860          0.2750   \n",
      "2              0.42450           0.4504                0.2430          0.3613   \n",
      "3              0.86630           0.6869                0.2575          0.6638   \n",
      "4              0.20500           0.4000                0.1625          0.2364   \n",
      "..                 ...              ...                   ...             ...   \n",
      "564            0.21130           0.4107                0.2216          0.2060   \n",
      "565            0.19220           0.3215                0.1628          0.2572   \n",
      "566            0.30940           0.3403                0.1418          0.2218   \n",
      "567            0.86810           0.9387                0.2650          0.4087   \n",
      "568            0.06444           0.0000                0.0000          0.2871   \n",
      "\n",
      "     fractal_dimension_worst  Unnamed: 32  \n",
      "0                    0.11890          NaN  \n",
      "1                    0.08902          NaN  \n",
      "2                    0.08758          NaN  \n",
      "3                    0.17300          NaN  \n",
      "4                    0.07678          NaN  \n",
      "..                       ...          ...  \n",
      "564                  0.07115          NaN  \n",
      "565                  0.06637          NaN  \n",
      "566                  0.07820          NaN  \n",
      "567                  0.12400          NaN  \n",
      "568                  0.07039          NaN  \n",
      "\n",
      "[569 rows x 33 columns]>\n"
     ]
    }
   ],
   "source": [
    "# multinomial Logistic regression for multiple features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/breast-cancer-wisconsin.csv\")\n",
    "\n",
    "x = X = df.drop(columns=[\"id\", \"diagnosis\", \"Unnamed: 32\"], errors=\"ignore\").to_numpy(\n",
    "    dtype=float\n",
    ")\n",
    "y_true = df[\"diagnosis\"].map({\"M\": 1, \"B\": 0}).to_numpy()\n",
    "y = np.eye(len(np.unique(y_true)))[y_true]\n",
    "\n",
    "def train_test_split_numpy(X, y, test_size=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    test_count = int(len(X) * test_size)\n",
    "\n",
    "    test_idx = indices[:test_count]\n",
    "    train_idx = indices[test_count:]\n",
    "\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    x = np.asarray(x)\n",
    "    if axis is None:\n",
    "        axis = x.ndim - 1\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def train_model(x, y, learning_rate, epochs=5000):\n",
    "    x = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "    unique_classes = y.shape[1]\n",
    "    n_dataset_col = x.shape[1]\n",
    "    w_vector = np.zeros((unique_classes, n_dataset_col))\n",
    "    bias = np.zeros((unique_classes, 1))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        z = x @ w_vector.T + bias.T\n",
    "\n",
    "        probabilities = softmax(z)\n",
    "\n",
    "        loss = -np.mean(np.sum(y * np.log(probabilities + 1e-9), axis=1))\n",
    "\n",
    "        if loss < 1e-6:\n",
    "            print(\"stopping the learning\")\n",
    "            break\n",
    "\n",
    "        p_y = probabilities - y\n",
    "        wieght_gradient = p_y.T @ x\n",
    "        bias_gradient = np.sum(p_y, axis=0, keepdims=True).T\n",
    "\n",
    "        w_vector = w_vector - (learning_rate * wieght_gradient)\n",
    "        bias = bias - (learning_rate * bias_gradient)\n",
    "    return w_vector, bias\n",
    "\n",
    "\n",
    "\n",
    "def predict(x, trained_weights, trained_bias):\n",
    "    x = (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "    z = x @ trained_weights.T + trained_bias.T\n",
    "    probabilities = softmax(z)\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_numpy(x, y)\n",
    "\n",
    "weights, bias = train_model(X_train, y_train, learning_rate=0.01)\n",
    "predictions = predict(X_test, weights, bias)\n",
    "\n",
    "y_test = np.argmax(y_test, axis=1) \n",
    "test_acc = np.mean(y_test == predictions)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "print(df.tail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9880c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iteration 17062, loss change < 1e-06 : 9.999003555104247e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Predictions: [1 2 3]'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordinal Logistic regression for multiple features\n",
    "import numpy as np\n",
    "\n",
    "X_dataset = np.array([[1, 5], [3, 3], [4, 1]])\n",
    "\n",
    "Y_dataset = np.array([[1], [2], [3]])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def train_model(X, Y, epochs=500000):\n",
    "    num_classes = len(np.unique(Y))\n",
    "    num_samples, num_features = X.shape\n",
    "    y_hot_encoded = np.eye(num_classes)[Y.flatten() - 1]\n",
    "    num_thresholds = y_hot_encoded.shape[1] - 1\n",
    "    w_vector = np.zeros((num_features))\n",
    "    bias = 0\n",
    "    learning_rate = 0.01\n",
    "    thresholds = np.cumsum(\n",
    "        np.exp(np.zeros(num_thresholds))\n",
    "    )  # Ensures thresholds are increasing\n",
    "    prev_loss = float('inf')\n",
    "    tolerance = 1e-6  # minimum change in loss\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Compute Latent Score (z)\n",
    "        z = X @ w_vector + bias\n",
    "\n",
    "        # Broadcoast thresholds (2,) against z (3,)\n",
    "        cum_probs = sigmoid(thresholds.reshape(1, -1) - z.reshape(-1, 1))\n",
    "\n",
    "        # Add a column of 1.0 for P(Y <= max_category)\n",
    "        cum_probs = np.hstack((cum_probs, np.ones((num_samples, 1))))\n",
    "\n",
    "        # Individual Probabilities: P(Y=j) = P(Y<=j) - P(Y<=j-1)\n",
    "        probs = np.diff(cum_probs, prepend=0, axis=1)\n",
    "\n",
    "        # compute loss\n",
    "        loss = -np.sum(y_hot_encoded * np.log(probs + 1e-15))\n",
    "\n",
    "        if abs(prev_loss - loss) < tolerance:\n",
    "            print(\n",
    "                f\"Stopping at iteration {i + 1}, loss change < {tolerance} : {abs(prev_loss - loss)}\"\n",
    "            )\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "        #  1{Y<=J} indicator function (1 if true, 0 if False); J is threshold\n",
    "        indicator = (Y <= np.arange(1, num_classes)).astype(int)\n",
    "        # ( P(Y<=J) - 1{Y<=J} )\n",
    "        error = np.delete(cum_probs, -1, axis=1) - indicator\n",
    "        # bias gradient sum of ( P(Y<=J) - 1{Y<=J} )\n",
    "        dz = -np.sum(error, axis=1)\n",
    "\n",
    "        # compute weight gradient sum of ( P(Y<=J) - 1{Y<=J} ) * X\n",
    "        dw = X.T @ dz\n",
    "        db = np.sum(dz)\n",
    "        d_thresholds = np.sum(error, axis=0)\n",
    "\n",
    "        # # # update the weight , bias & thresholds\n",
    "        w_vector = w_vector - learning_rate * dw\n",
    "        bias = bias - learning_rate * db\n",
    "        thresholds = np.sort(thresholds - learning_rate * d_thresholds)\n",
    "    return w_vector, bias, thresholds\n",
    "\n",
    "\n",
    "def predict(new_datatset, w, b, thresholds):\n",
    "    # Compute Latent Score (z)\n",
    "    z = new_datatset @ w + b\n",
    "\n",
    "    # Broadcoast thresholds (2,) against z (3,)\n",
    "    cum_probs = sigmoid(thresholds.reshape(1, -1) - z.reshape(-1, 1))\n",
    "\n",
    "    # Add a column of 1.0 for P(Y <= max_category)\n",
    "    cum_probs = np.hstack((cum_probs, np.ones((new_datatset.shape[0], 1))))\n",
    "\n",
    "    # Individual Probabilities: P(Y=j) = P(Y<=j) - P(Y<=j-1)\n",
    "    probs = np.diff(cum_probs, prepend=0, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1) + 1\n",
    "    return f\"Predictions: {predictions}\"\n",
    "\n",
    "w_vector, bias, thresholds = train_model(X_dataset, Y_dataset)\n",
    "X_test = np.array([[2, 4]])\n",
    "predict(X_dataset, w_vector, bias, thresholds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
