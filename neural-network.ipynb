{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc85349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:57:16.256104Z",
     "start_time": "2026-02-05T11:57:16.221502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Probability: 0.4035\n"
     ]
    }
   ],
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers=[2, 2, 2, 1]):  # [input_size, hidden_layer1_size, hidden_layer2_size, ..., output_size]\n",
    "        # Dataset (Inputs: 2, Samples: 3)\n",
    "        self.x_dataset = np.array([[2, 6], [8, 5], [1, 2]])\n",
    "        self.y_value = np.array([[1], [1], [0]])\n",
    "\n",
    "        self.x_weights = []\n",
    "        self.x_bias = []\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            # Xavier Initialization: np.sqrt(1 / nodes_in)\n",
    "            limit = np.sqrt(1 / layers[i])\n",
    "            w = np.random.uniform(-limit, limit, (layers[i+1], layers[i]))\n",
    "            b = np.zeros(layers[i+1])\n",
    "            \n",
    "            self.x_weights.append(w)\n",
    "            self.x_bias.append(b)\n",
    "\n",
    "       \n",
    "        self.output_weights = self.x_weights.pop()\n",
    "        self.output_bias = self.x_bias.pop()\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.tolerance = 1e-6  # minimum change in loss\n",
    "        self.prev_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # xavier intialization of the vanishing gradients\n",
    "    @staticmethod\n",
    "    def xavier_intialization():\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_sgimoid(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def predict(self, new_data):\n",
    "        dataset = np.atleast_2d(new_data)\n",
    "\n",
    "        for w,b in zip(self.x_weights, self.x_bias):\n",
    "            neuron = (dataset @ w.T) + b\n",
    "            dataset = self.sigmoid(neuron)\n",
    "\n",
    "        return self.sigmoid(dataset @ self.output_weights.T + self.output_bias)\n",
    "    \n",
    "    def train(self, epochs = 5):\n",
    "         \n",
    "        for i in range(epochs):\n",
    "            activations, prediction = self.forward_propagation()\n",
    "\n",
    "            # - (true value * log( predited value ) + (1 - true value)  * log(1 - predicted value) )\n",
    "            loss = -np.mean(self.y_value * np.log(prediction) + (1 - self.y_value) * np.log(1 - prediction))\n",
    "\n",
    "            if abs(self.prev_loss - loss) < self.tolerance:\n",
    "                print(f\"Stopping at iteration {i + 1}, loss change < {self.tolerance} : {abs(self.prev_loss - loss)}\")\n",
    "                break\n",
    "            \n",
    "            self.back_propagation(activations, prediction)\n",
    "            self.prev_loss = loss\n",
    "        return prediction\n",
    "\n",
    "                \n",
    "    # FORWARD PROPAGATION \n",
    "    def forward_propagation(self):\n",
    "        dataset = self.x_dataset\n",
    "        activations = []  # Store all hidden layers activations values\n",
    "\n",
    "       \n",
    "        for w,b in zip(self.x_weights, self.x_bias): # Basically getting the length of the list\n",
    "            neuron = ( dataset @ w.T) + b  # ( Hidden layer Weights * Inputs ) + Bias\n",
    "            activation = self.sigmoid(neuron)  # Sigmoid ( neuron )\n",
    "            dataset = activation # Re-assign the input as the activation of the previous layer \n",
    "            activations.append(dataset) # Collecting all activations of each layers\n",
    "\n",
    "        # Sigmoid ( ( output_weights * last layer activation ) + output_bias )\n",
    "        prediction = self.sigmoid(dataset @ self.output_weights.T + self.output_bias)\n",
    "        return activations, prediction\n",
    "\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    def back_propagation(self, activations, prediction):\n",
    "        batch_size = self.x_dataset.shape[0]\n",
    "        \n",
    "        # output delta = predicted value - true value\n",
    "        output_delta = prediction - self.y_value\n",
    "\n",
    "        # output delta * last activation value / batch_size\n",
    "        output_dw = (output_delta.T @ activations[-1]) / batch_size\n",
    "        output_db = np.sum(output_delta, axis=0) / batch_size # output gradient bias\n",
    "\n",
    "\n",
    "        hidden_dw = []\n",
    "        hidden_db = []\n",
    "        delta = output_delta\n",
    "\n",
    "        # calculate the backdrop error and gradient of each hidden layer = W.T * delta * derivative of sigmoid (activation)\n",
    "        for i in reversed(range(len(self.x_weights))):\n",
    "            if i == len(self.x_weights) - 1:\n",
    "                # last hidden layer -> output\n",
    "                delta = (delta @ self.output_weights) * self.derivative_sgimoid(activations[i])\n",
    "            else:\n",
    "                delta = (delta @ self.x_weights[i + 1]) * self.derivative_sgimoid(activations[i])\n",
    "            \n",
    "\n",
    "            prev_activation = self.x_dataset if i == 0 else activations[i - 1]\n",
    "            hidden_dw.insert(0, ((delta.T @ prev_activation) / batch_size))\n",
    "            hidden_db.insert(0, np.sum(delta, axis=0) / batch_size)\n",
    "\n",
    "        # update the output weight and bias = old weight - ( learning_rate * gradient )\n",
    "        self.output_weights -= self.learning_rate * output_dw \n",
    "        self.output_bias -= self.learning_rate * output_db\n",
    "\n",
    "         # update the hidden layer weight and bias = old weight - ( learning_rate * gradient )\n",
    "        for i in range(len(self.x_weights)):\n",
    "            self.x_weights[i] -= self.learning_rate * hidden_dw[i]\n",
    "            self.x_bias[i] -= self.learning_rate * hidden_db[i]\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "train = NeuralNetwork()\n",
    "train.train(epochs=1)\n",
    "\n",
    "test_pass = train.predict([8, 6])\n",
    "print(f\"Pass Probability: {test_pass[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26511dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "x = np.array([\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([1, 0, 0])\n",
    "\n",
    "class Mnist:\n",
    "\n",
    "    def __init__(self, input_size=784, output_size=10, layers=2, hidden_size=128):\n",
    "        self.layers = layers\n",
    "        self.w = [\n",
    "            np.random.randn(hidden_size, input_size if i == 0 else hidden_size)\n",
    "            * np.sqrt(2.0 / (input_size if i == 0 else hidden_size))\n",
    "            for i in range(layers)\n",
    "        ]   \n",
    "        self.b = [np.zeros(hidden_size) for _ in range(layers)]\n",
    "        self.output_w = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.output_b = np.zeros(output_size)\n",
    "        self.learning_rate = 0.01\n",
    "        self.prev_loss = float('inf')\n",
    "        self.tolerance = 1e-6 \n",
    "\n",
    "    @classmethod\n",
    "    def relu(cls,x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @classmethod\n",
    "    def derivative_relu(cls, x):\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    @classmethod\n",
    "    def softmax(cls, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        self.pre_activations = []\n",
    "        self.activations = []\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            z = x @ w.T + b\n",
    "            x = self.relu(z)\n",
    "            self.pre_activations.append(z)\n",
    "            self.activations.append(x)\n",
    "        output = x @ self.output_w.T + self.output_b\n",
    "        return self.softmax(output)\n",
    "\n",
    "\n",
    "    def back_propagation(self, x_inputs, y_prediction, labels):\n",
    "        batch_size = labels.shape[0]\n",
    "        delta = y_prediction - labels\n",
    "        \n",
    "        self.output_w -= self.learning_rate  * (delta.T @ self.activations[-1] / batch_size)\n",
    "        self.output_b -= self.learning_rate * (np.sum(delta, axis=0) / batch_size)\n",
    "\n",
    "        for x in reversed(range(len(self.w))):\n",
    "            w = self.output_w if x == len(self.w) - 1 else self.w[x + 1] \n",
    "            delta = delta @ w * self.derivative_relu(self.pre_activations[x])\n",
    "\n",
    "            activations = x_inputs if x == 0 else self.activations[x - 1]\n",
    "            \n",
    "            self.w[x] -= self.learning_rate * ( delta.T @ activations / batch_size)\n",
    "            self.b[x] -= self.learning_rate * ( np.sum(delta, axis=0) / batch_size )\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=64, epochs=10):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                prediction = self.forward_propagation(x_batch)\n",
    "\n",
    "                # - (true value * log( predited value ) + (1 - true value)  * log(1 - predicted value) )\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(prediction + 1e-8), axis=1))\n",
    "\n",
    "                self.back_propagation(x_batch, prediction, y_batch)\n",
    "\n",
    "                print(f\"Neural Network --- Epoch {epoch} - Batch {i} : Loss is {loss}\")\n",
    "\n",
    "\n",
    "    def predict(self,x_test, y_test):\n",
    "        pred = self.forward_propagation(x_test)\n",
    "        predicted_digit = np.argmax(pred, axis=1)\n",
    "        true_digit = np.argmax(y_test, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_digit == true_digit)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    def predict_upload(self, image_path):\n",
    "        img = Image.open(image_path).convert(\"L\")  # grayscale\n",
    "\n",
    "        img = img.resize((28, 28))  # MNIST size\n",
    "        img_array = np.array(img)\n",
    "\n",
    "        # Invert colors if necessary (MNIST is white digit on black)\n",
    "        if np.mean(img_array) > 127:  # simple heuristic\n",
    "            img_array = 255 - img_array\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        img_array = img_array / 255.0\n",
    "        # Flatten and add batch dimension\n",
    "        img_array = img_array.reshape(1, 784)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        pred = self.forward_propagation(img_array)\n",
    "        print({i: x for i, x in enumerate(pred.flatten())})\n",
    "        digit = np.argmax(pred)\n",
    "\n",
    "        # Visualize\n",
    "        plt.imshow(img_array.reshape(28, 28), cmap=\"gray\")\n",
    "        plt.title(f\"Predicted: {digit}.\")\n",
    "        plt.show()\n",
    "\n",
    "model = Mnist()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "y_train_oh = np.eye(10)[y_train]\n",
    "y_test_oh = np.eye(10)[y_test]\n",
    "model.train(x_train, y_train_oh, epochs=10)\n",
    "model.predict(x_test, y_test_oh)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "abff5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: np.float64(0.0002704615710358798), 1: np.float64(2.7722647012191934e-07), 2: np.float64(0.02452540906464338), 3: np.float64(0.9639273319117788), 4: np.float64(7.414652681956413e-08), 5: np.float64(0.0023668620206132307), 6: np.float64(2.120418500091131e-06), 7: np.float64(3.187320955840273e-05), 8: np.float64(0.008740021544578987), 9: np.float64(0.00013556888629440427)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.predict_upload('number-images/three.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
