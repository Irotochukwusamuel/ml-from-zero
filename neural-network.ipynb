{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc85349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Probability: 0.5056\n"
     ]
    }
   ],
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers=[2, 2, 2, 1]):  # [input_size, hidden_layer1_size, hidden_layer2_size, ..., output_size]\n",
    "        # Dataset (Inputs: 2, Samples: 3)\n",
    "        self.x_dataset = np.array([[2, 6], [8, 5], [1, 2]])\n",
    "        self.y_value = np.array([[1], [1], [0]])\n",
    "\n",
    "        self.x_weights = []\n",
    "        self.x_bias = []\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            # Xavier Initialization: np.sqrt(1 / nodes_in)\n",
    "            limit = np.sqrt(1 / layers[i])\n",
    "            w = np.random.uniform(-limit, limit, (layers[i+1], layers[i]))\n",
    "            b = np.zeros(layers[i+1])\n",
    "            \n",
    "            self.x_weights.append(w)\n",
    "            self.x_bias.append(b)\n",
    "\n",
    "       \n",
    "        self.output_weights = self.x_weights.pop()\n",
    "        self.output_bias = self.x_bias.pop()\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.tolerance = 1e-6  # minimum change in loss\n",
    "        self.prev_loss = float('inf')\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_sgimoid(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def predict(self, new_data):\n",
    "        dataset = np.atleast_2d(new_data)\n",
    "\n",
    "        for w,b in zip(self.x_weights, self.x_bias):\n",
    "            neuron = (dataset @ w.T) + b\n",
    "            dataset = self.sigmoid(neuron)\n",
    "\n",
    "        return self.sigmoid(dataset @ self.output_weights.T + self.output_bias)\n",
    "    \n",
    "    def train(self, epochs = 5):\n",
    "         \n",
    "        for i in range(epochs):\n",
    "            activations, prediction = self.forward_propagation()\n",
    "\n",
    "            # - (true value * log( predited value ) + (1 - true value)  * log(1 - predicted value) )\n",
    "            loss = -np.mean(self.y_value * np.log(prediction) + (1 - self.y_value) * np.log(1 - prediction))\n",
    "\n",
    "            if abs(self.prev_loss - loss) < self.tolerance:\n",
    "                print(f\"Stopping at iteration {i + 1}, loss change < {self.tolerance} : {abs(self.prev_loss - loss)}\")\n",
    "                break\n",
    "            \n",
    "            self.back_propagation(activations, prediction)\n",
    "            self.prev_loss = loss\n",
    "        return prediction\n",
    "\n",
    "                \n",
    "    # FORWARD PROPAGATION \n",
    "    def forward_propagation(self):\n",
    "        dataset = self.x_dataset\n",
    "        activations = []  # Store all hidden layers activations values\n",
    "\n",
    "       \n",
    "        for w,b in zip(self.x_weights, self.x_bias): # Basically getting the length of the list\n",
    "            neuron = ( dataset @ w.T) + b  # ( Hidden layer Weights * Inputs ) + Bias\n",
    "            activation = self.sigmoid(neuron)  # Sigmoid ( neuron )\n",
    "            dataset = activation # Re-assign the input as the activation of the previous layer \n",
    "            activations.append(dataset) # Collecting all activations of each layers\n",
    "\n",
    "        # Sigmoid ( ( output_weights * last layer activation ) + output_bias )\n",
    "        prediction = self.sigmoid(dataset @ self.output_weights.T + self.output_bias)\n",
    "        return activations, prediction\n",
    "\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    def back_propagation(self, activations, prediction):\n",
    "        batch_size = self.x_dataset.shape[0]\n",
    "        \n",
    "        # output delta = predicted value - true value\n",
    "        output_delta = prediction - self.y_value\n",
    "\n",
    "        # output delta * last activation value / batch_size\n",
    "        output_dw = (output_delta.T @ activations[-1]) / batch_size\n",
    "        output_db = np.sum(output_delta, axis=0) / batch_size # output gradient bias\n",
    "\n",
    "\n",
    "        hidden_dw = []\n",
    "        hidden_db = []\n",
    "        delta = output_delta\n",
    "\n",
    "        # calculate the backdrop error and gradient of each hidden layer = W.T * delta * derivative of sigmoid (activation)\n",
    "        for i in reversed(range(len(self.x_weights))):\n",
    "            if i == len(self.x_weights) - 1:\n",
    "                # last hidden layer -> output\n",
    "                delta = (delta @ self.output_weights) * self.derivative_sgimoid(activations[i])\n",
    "            else:\n",
    "                delta = (delta @ self.x_weights[i + 1]) * self.derivative_sgimoid(activations[i])\n",
    "            \n",
    "\n",
    "            prev_activation = self.x_dataset if i == 0 else activations[i - 1]\n",
    "            hidden_dw.insert(0, ((delta.T @ prev_activation) / batch_size))\n",
    "            hidden_db.insert(0, np.sum(delta, axis=0) / batch_size)\n",
    "\n",
    "        # update the output weight and bias = old weight - ( learning_rate * gradient )\n",
    "        self.output_weights -= self.learning_rate * output_dw \n",
    "        self.output_bias -= self.learning_rate * output_db\n",
    "\n",
    "         # update the hidden layer weight and bias = old weight - ( learning_rate * gradient )\n",
    "        for i in range(len(self.x_weights)):\n",
    "            self.x_weights[i] -= self.learning_rate * hidden_dw[i]\n",
    "            self.x_bias[i] -= self.learning_rate * hidden_db[i]\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "train = NeuralNetwork()\n",
    "train.train(epochs=1)\n",
    "\n",
    "test_pass = train.predict([8, 6])\n",
    "print(f\"Pass Probability: {test_pass[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3cc39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
