{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc85349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:57:16.256104Z",
     "start_time": "2026-02-05T11:57:16.221502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass Probability: 0.4035\n"
     ]
    }
   ],
   "source": [
    "# Multi-Layer Perceptron Neural Network\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers=[2, 2, 2, 1]):  # [input_size, hidden_layer1_size, hidden_layer2_size, ..., output_size]\n",
    "        # Dataset (Inputs: 2, Samples: 3)\n",
    "        self.x_dataset = np.array([[2, 6], [8, 5], [1, 2]])\n",
    "        self.y_value = np.array([[1], [1], [0]])\n",
    "\n",
    "        self.x_weights = []\n",
    "        self.x_bias = []\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            # Xavier Initialization: np.sqrt(1 / nodes_in)\n",
    "            limit = np.sqrt(1 / layers[i])\n",
    "            w = np.random.uniform(-limit, limit, (layers[i+1], layers[i]))\n",
    "            b = np.zeros(layers[i+1])\n",
    "            \n",
    "            self.x_weights.append(w)\n",
    "            self.x_bias.append(b)\n",
    "\n",
    "       \n",
    "        self.output_weights = self.x_weights.pop()\n",
    "        self.output_bias = self.x_bias.pop()\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.tolerance = 1e-6  # minimum change in loss\n",
    "        self.prev_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # xavier intialization of the vanishing gradients\n",
    "    @staticmethod\n",
    "    def xavier_intialization():\n",
    "        pass\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_sgimoid(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def predict(self, new_data):\n",
    "        dataset = np.atleast_2d(new_data)\n",
    "\n",
    "        for w,b in zip(self.x_weights, self.x_bias):\n",
    "            neuron = (dataset @ w.T) + b\n",
    "            dataset = self.sigmoid(neuron)\n",
    "\n",
    "        return self.sigmoid(dataset @ self.output_weights.T + self.output_bias)\n",
    "    \n",
    "    def train(self, epochs = 5):\n",
    "         \n",
    "        for i in range(epochs):\n",
    "            activations, prediction = self.forward_propagation()\n",
    "\n",
    "            # - (true value * log( predited value ) + (1 - true value)  * log(1 - predicted value) )\n",
    "            loss = -np.mean(self.y_value * np.log(prediction) + (1 - self.y_value) * np.log(1 - prediction))\n",
    "\n",
    "            if abs(self.prev_loss - loss) < self.tolerance:\n",
    "                print(f\"Stopping at iteration {i + 1}, loss change < {self.tolerance} : {abs(self.prev_loss - loss)}\")\n",
    "                break\n",
    "            \n",
    "            self.back_propagation(activations, prediction)\n",
    "            self.prev_loss = loss\n",
    "        return prediction\n",
    "\n",
    "                \n",
    "    # FORWARD PROPAGATION \n",
    "    def forward_propagation(self):\n",
    "        dataset = self.x_dataset\n",
    "        activations = []  # Store all hidden layers activations values\n",
    "\n",
    "       \n",
    "        for w,b in zip(self.x_weights, self.x_bias): # Basically getting the length of the list\n",
    "            neuron = ( dataset @ w.T) + b  # ( Hidden layer Weights * Inputs ) + Bias\n",
    "            activation = self.sigmoid(neuron)  # Sigmoid ( neuron )\n",
    "            dataset = activation # Re-assign the input as the activation of the previous layer \n",
    "            activations.append(dataset) # Collecting all activations of each layers\n",
    "\n",
    "        # Sigmoid ( ( output_weights * last layer activation ) + output_bias )\n",
    "        prediction = self.sigmoid(dataset @ self.output_weights.T + self.output_bias)\n",
    "        return activations, prediction\n",
    "\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "    def back_propagation(self, activations, prediction):\n",
    "        batch_size = self.x_dataset.shape[0]\n",
    "        \n",
    "        # output delta = predicted value - true value\n",
    "        output_delta = prediction - self.y_value\n",
    "\n",
    "        # output delta * last activation value / batch_size\n",
    "        output_dw = (output_delta.T @ activations[-1]) / batch_size\n",
    "        output_db = np.sum(output_delta, axis=0) / batch_size # output gradient bias\n",
    "\n",
    "\n",
    "        hidden_dw = []\n",
    "        hidden_db = []\n",
    "        delta = output_delta\n",
    "\n",
    "        # calculate the backdrop error and gradient of each hidden layer = W.T * delta * derivative of sigmoid (activation)\n",
    "        for i in reversed(range(len(self.x_weights))):\n",
    "            if i == len(self.x_weights) - 1:\n",
    "                # last hidden layer -> output\n",
    "                delta = (delta @ self.output_weights) * self.derivative_sgimoid(activations[i])\n",
    "            else:\n",
    "                delta = (delta @ self.x_weights[i + 1]) * self.derivative_sgimoid(activations[i])\n",
    "            \n",
    "\n",
    "            prev_activation = self.x_dataset if i == 0 else activations[i - 1]\n",
    "            hidden_dw.insert(0, ((delta.T @ prev_activation) / batch_size))\n",
    "            hidden_db.insert(0, np.sum(delta, axis=0) / batch_size)\n",
    "\n",
    "        # update the output weight and bias = old weight - ( learning_rate * gradient )\n",
    "        self.output_weights -= self.learning_rate * output_dw \n",
    "        self.output_bias -= self.learning_rate * output_db\n",
    "\n",
    "         # update the hidden layer weight and bias = old weight - ( learning_rate * gradient )\n",
    "        for i in range(len(self.x_weights)):\n",
    "            self.x_weights[i] -= self.learning_rate * hidden_dw[i]\n",
    "            self.x_bias[i] -= self.learning_rate * hidden_db[i]\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "train = NeuralNetwork()\n",
    "train.train(epochs=1)\n",
    "\n",
    "test_pass = train.predict([8, 6])\n",
    "print(f\"Pass Probability: {test_pass[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26511dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gt/lml8wvhx4mnbfhyd54zlg6s80000gn/T/ipykernel_8337/3662055213.py:41: RuntimeWarning: overflow encountered in matmul\n",
      "  z = x @ w.T + b\n",
      "/var/folders/gt/lml8wvhx4mnbfhyd54zlg6s80000gn/T/ipykernel_8337/3662055213.py:33: RuntimeWarning: invalid value encountered in subtract\n",
      "  e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "x = np.array([\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([1, 0, 0])\n",
    "\n",
    "class Mnist:\n",
    "\n",
    "    def __init__(self, input_size=784, output_size=10, layers=2, hidden_size=128):\n",
    "        self.layers = layers\n",
    "        self.w = [np.random.randn(hidden_size, input_size) if i == 0 else np.random.randn(hidden_size, hidden_size) for i in range(layers)]\n",
    "        self.b = [np.zeros(hidden_size) for _ in range(layers)]\n",
    "        self.output_w = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.output_b = np.zeros(output_size)\n",
    "        self.learning_rate = 0.01\n",
    "        self.prev_loss = float('inf')\n",
    "        self.tolerance = 1e-6  # minimum change in loss\n",
    "\n",
    "    @classmethod\n",
    "    def relu(cls,x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @classmethod\n",
    "    def derivative_relu(cls, x):\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    @classmethod\n",
    "    def softmax(cls, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        self.pre_activations = []\n",
    "        self.activations = []\n",
    "        for w, b in zip(self.w, self.b):\n",
    "            z = x @ w.T + b\n",
    "            x = self.relu(z)\n",
    "            self.pre_activations.append(z)\n",
    "            self.activations.append(x)\n",
    "            \n",
    "        output = x @ self.output_w.T + self.output_b\n",
    "        return self.softmax(output)\n",
    "\n",
    "\n",
    "    def back_propagation(self, x_inputs, y_prediction, labels):\n",
    "        batch_size = labels.shape[0]\n",
    "        delta = y_prediction - labels\n",
    "        \n",
    "        self.output_w -= self.learning_rate  * (delta.T @ self.activations[-1] / batch_size)\n",
    "        self.output_b -= self.learning_rate * (np.sum(delta, axis=0) / batch_size)\n",
    "\n",
    "        for x in reversed(range(len(self.w))):\n",
    "            \n",
    "            if x == len(self.w) - 1:\n",
    "                delta = delta @ self.output_w  * self.derivative_relu(self.pre_activations[x])\n",
    "            else:\n",
    "                delta = delta @ self.w[x + 1] * self.derivative_relu(self.pre_activations[x])\n",
    "            \n",
    "            activations = x_inputs if x == 0 else self.activations[x - 1]\n",
    "            \n",
    "            self.w[x] -= self.learning_rate * ( delta.T @ activations / batch_size)\n",
    "            self.b[x] -= self.learning_rate * ( np.sum(delta, axis=0) / batch_size )\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size=64, epochs=10):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for i in range(0, len(x_train), batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                prediction = self.forward_propagation(x_batch)\n",
    "\n",
    "                # - (true value * log( predited value ) + (1 - true value)  * log(1 - predicted value) )\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(prediction + 1e-8), axis=1))\n",
    "\n",
    "                self.back_propagation(x_batch, prediction, y_batch)\n",
    "\n",
    "\n",
    "    def predict(self,x_test, y_test):\n",
    "        pred = self.forward_propagation(x_test)\n",
    "        predicted_digit = np.argmax(pred, axis=1)\n",
    "        true_digit = np.argmax(y_test, axis=1)\n",
    "\n",
    "        accuracy = np.mean(predicted_digit == true_digit)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "model = Mnist()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784) / 255.0\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "y_train_oh = np.eye(10)[y_train]\n",
    "y_test_oh = np.eye(10)[y_test]\n",
    "model.train(x_train, y_train_oh, epochs=5)\n",
    "model.predict(x_test, y_test_oh)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
